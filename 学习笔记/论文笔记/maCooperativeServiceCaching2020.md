# Cooperative Service Caching and Workload Scheduling in Mobile Edge Computing

```bibtex
@inproceedings{maCooperativeServiceCaching2020,
  title = {Cooperative {{Service Caching}} and {{Workload Scheduling}} in {{Mobile Edge Computing}}},
  booktitle = {{{IEEE INFOCOM}} 2020 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Ma, Xiao and Zhou, Ao and Zhang, Shan and Wang, Shangguang},
  year = {2020},
  month = jul,
  pages = {2076--2085},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/INFOCOM41043.2020.9155455},
  isbn = {978-1-72816-412-0},
  language = {en}
}
```

## 要解决的问题：边缘服务器之间没有互相合作导致的边缘服务器的资源利用率不足

* 边缘服务器资源配置的不同造成的利用率不足
  * 例如，一个算力不足的边缘服务器和一个存储容量不足的边缘服务器在遇到计算任务或缓存任务时，只能把任务交给云计算中心，而不是互相交换任务
* 边缘服务器资源配置的不合理造成的资源浪费
  * 例如，给一个算力很强的边缘服务器配了一个很小的内存会使得边缘服务器只能一次执行少量计算任务，而不能一次下载多个任务执行

### 研究对象和问题

* 在一个LAN内网环境下存在多个边缘服务器，每个边缘服务器的算力和存储容量不尽相同
* 在LAN外有云计算中心
* 计算能力不足或没有缓存所请求服务的边缘服务器可以将自己的请求处理任务卸载到旁边的未满载边缘服务器或云端
* 任务的卸载和执行一样需要消耗时间，在处理时需要平衡
* 两个子问题：服务缓存和任务卸载及它们之间的相互作用
  * 任务只有在缓存了对应服务的机器上才能运行
  * 因此服务缓存策略决定了任务可以卸载的位置
  * 进而任务卸载的结果也可以看出一个服务缓存策略效果如何

## 研究方法

* 两个子问题建模为一个mixed integer nonlinear programming
problem，设计一个two-layer Iterative Caching updatE (ICE) algorithm处理服务缓存和任务卸载的相互作用
* 用排队模型评估任务卸载和执行的耗时，进而评估系统内任务的平均响应时间，进而可以通过最小化平均响应时间的方法求出任务卸载耗时和任务执行耗时的平衡点
* 针对边缘服务器算力和存储容量各不相同的问题，利用工作量调度子问题的凸性，并基于注水思想提出了一种具有多项式计算复杂度的启发式工作量调度算法

## 创新点

* 研究了在边缘端互相合作的情况下的服务缓存和任务调度问题，将问题建模为一个最小化服务响应时间的mixed integer nonlinear programming问题，并且证明了它的非多项式复杂度
* 证明了任务卸载子问题的凸性，使用排队模型分析了系统各处的延迟
* 设计了一个two-layer Iterative Caching updatE (ICE) algorithm解决了最小化服务响应时间和外网流量的问题

## 相关研究

* 多边缘服务器共同优化服务缓存和工作负载调度：在服务缓存成本预算限制范围内最大化边缘处理的请求数量

缺点：

* 实际情况下很难确定服务缓存成本预算
* 没有考虑在边缘计算中最为重要的指标——服务响应时间

### 任务卸载

* 在单跳和多跳边缘服务器间的任务卸载
* 云-边之间的在线任务调度
* 分级结构中的任务卸载

缺点：

* 假定边缘服务器缓存了所有服务，不符合实际

### 服务缓存

* 基于受欢迎程度的分布式缓存算法
* 以用户为中心的边缘缓存机制，每个用户都由多个边缘共同提供服务
* 基于位置的缓存策略，通过预测内容的受欢迎程度最大化缓存命中率
* 基于预测的缓存放置策略
* 基于历史的动态边缘缓存

缺点：

* 计算和服务缓存不应该·割裂来看

### 任务卸载和服务缓存的联合优化

* 针对数据密集型应用（例如视频）的优化不能直接应用于数据和计算密集型应用（例如AR/VR）中
* 一些研究只能应用于多基站覆盖用户的情况
* 一些研究没有考虑边缘端协作
* 还有一些研究没有把服务响应时间作为性能评价指标因而不能展现边缘计算给用户带来的好处

## 建模

### 边缘缓存和任务规划策略

#### 边缘缓存策略

$$
\bm{C}=(c_{ns}\in\{0,1\}:n\in\mathbb{N},s\in\mathbb{S})
$$

* $\bm{C}$：一个矩阵表示边缘缓存策略
* $c_{ns}$：是否在边缘服务器$n$上缓存服务$s$
* $n$：表示一个边缘服务器
* $\mathbb{N}$：在一个LAN下的所有边缘服务器组成的集合
* $s$：表示一个服务
* $\mathbb{S}$：所有要部署的服务的集合

#### 边缘服务器存储容量限制

$$
\sum_{s\in\mathbb{S}}c_{ns}p_s\leq P_n
$$

* $p_s$：服务$s$的大小
* $P_n$：边缘服务器$n$的存储容量

#### 任务规划策略

$$
\bm\Lambda=(\lambda_{ns}\in[0,1]:n\in\mathbb{N}\cup\{\omicron\},s\in\mathbb{S})
$$

* $\bm\Lambda$：一个矩阵表示任务规划策略
* $\lambda_{ns}$：边缘服务器$n$上分担的服务$s$的计算量比率
* $\lambda_{\omicron s}$：云服务器上分担的服务$s$的计算量比率

#### 任务规划限制

##### 每个任务都要完成

$$
\sum_{n\in\mathbb{N}\cup\{\omicron\}}\lambda_{ns}=1
$$

##### 边缘服务器实际计算量不会比分配给它和它周围边缘服务器的计算量还大

$$
\lambda_{ns}A_s\leq\sum_{i\in\Theta_n\cup\{n\}}A_{is}
$$

* $A_s$：假定请求的到达过程为泊松过程，$A_s$是服务$s$的请求的到达强度
  * 单位时间内服务$s$的请求到达次数的期望
* $\Theta_n$：边缘服务器$n$相连的周围的服务器集合
* $A_{ns}$：单位时间内到达边缘服务器$n$的服务$s$的请求数量的期望

注：帮助理解到达的请求量$A_{ns}$和实际的请求量$\lambda_{ns}A_s$

以下以“请求量”指代单位时间内请求到达次数的期望

* $\lambda_{ns}A_s$表示边缘服务器$n$上实际所处理的服务$s$的请求量
* 如果$\lambda_{ns}A_s\leq A_{ns}$：
  * 说明分配给边缘服务器$n$的服务$s$的请求量$A_{ns}$比它实际运行的服务$s$的请求量$\lambda_{ns}A_s$还大
  * 说明有部分服务$s$的请求量被卸载到其他边缘服务器了
  * 这时$A_{ns}-\lambda_{ns}A_s$就表示边缘服务器$n$卸载到其他边缘服务器的请求量
* 反之，如果$\lambda_{ns}A_s\geq A_{ns}$：
  * 说明分配给边缘服务器$n$的服务$s$的请求量$A_{ns}$还没有它实际运行的服务$s$的请求量$\lambda_{ns}A_s$大
  * 说明有其他边缘服务器的服务$s$的请求量被卸载到了边缘服务器$n$
  * 这时$\lambda_{ns}A_s-A_{ns}$就表示从其他边缘服务器卸载过来的服务$s$的请求量

### 服务延迟

$$
\frac{1}{\mu_{ns}}=\frac{\beta_s}{r_{ns}}
$$

* $\beta_s$：假定服务$s$的计算任务在任意CPU时间片内发出“计算请求”的次数服从指数分布，期望值为$\beta_s$
  * 即处理服务$s$所需的平均计算量
* $r_{ns}$：边缘服务器$n$任意CPU时间片内可以处理的服务$s$的“计算请求”数量
  * 单位时间内可以为处理服务$s$的请求提供的计算量
  * 即分配给服务$s$的计算资源量
* 由于服务请求在任意CPU时间片内的到达次数服从指数分布，而边缘服务器$n$分配给服务$s$的计算资源量固定为$r_{ns}$，因此边缘服务器$n$处理服务$s$的请求所要花费时间也服从指数分布，均值即$\frac{\beta_s}{r_{ns}}$
* $\mu_{ns}$：边缘服务器$n$单位时间内能处理的服务$s$的请求数量
  * 由于每个服务请求的处理时间服从指数分布，均值为$\frac{\beta_s}{r_{ns}}$，那么单位时间内能处理的服务请求量则为$\frac{r_{ns}}{\beta_s}$
  * **（为什么？边缘服务器处理服务请求所花的时间为指数分布，单位时间内能处理的服务请求量是它的倒数，那指数分布的随机变量的倒数的分布是什么分布？）**

$$
D_{ns}=\frac{1}{\mu_{ns}-\lambda_{ns}A_s}
$$

* $D_{ns}$：请求处理的延迟
  * 似乎需要用到排队论的知识，暂时不懂

#### 服务延迟限制

##### 保持队列的稳定性

$$
\lambda_{ns}A_s<\mu_{ns}
$$

* 似乎需要用到排队论的知识，暂时不懂

## 思考

貌似可行：

* 边缘之间连接不稳定的情况？
* 边缘服务器服务按需动态部署的情况？
* 边缘服务器之间不能一跳连接的情况？
* 本文似乎假定任务少的时候和任务多的时候的任务处理速度符合同样的分布，但是实际情况下，如果任务少了，单位时间内一个任务被分配的CPU时间会增加，任务处理速度因此会加快
  * 相关疑惑点：本文是假定边缘服务器一个个运行队伍里的任务还是说所有到达的任务都一起加入CPU轮转处理？
* $\beta_s$的定义似乎说明本文假定了同一服务的不同请求可能有不同的计算量且这些计算量是连续分布的，实际情况下不同请求的计算量可能集中分布于多个值处（例如分布式神经网络同一步的计算量大致相同，不同步的计算量差别很大）

感觉不太行：

* 到达概率$A_s$不能提前确定的情况？$A_s$如何确定？$A_s$不断变化的情况？
* 服务可以被部分缓存的情况（$c_{ns}\in[0,1]$，比如边缘智能缓存部分神经网络层）？一个任务需要多个服务器处理的情况？